{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load and process the data\n",
    "df = pd.read_csv('Updated_Common_Data_Sains.csv')\n",
    "\n",
    "# Convert date from 'YYYYMMDD' to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "\n",
    "# Extract features from date\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.dayofweek\n",
    "\n",
    "# Drop the original date column\n",
    "df = df.drop(columns=['date','own_brand'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supermarket</th>\n",
       "      <th>Sains_price</th>\n",
       "      <th>unit</th>\n",
       "      <th>names</th>\n",
       "      <th>category</th>\n",
       "      <th>ASDA_price</th>\n",
       "      <th>Morrisons_price</th>\n",
       "      <th>Tesco_price</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sains</td>\n",
       "      <td>6.000</td>\n",
       "      <td>unit</td>\n",
       "      <td>Vuse Go</td>\n",
       "      <td>home</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sains</td>\n",
       "      <td>6.000</td>\n",
       "      <td>unit</td>\n",
       "      <td>Vuse Go Edition</td>\n",
       "      <td>home</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sains</td>\n",
       "      <td>5.425</td>\n",
       "      <td>kg</td>\n",
       "      <td>Heinz Tomato Ketchup</td>\n",
       "      <td>food_cupboard</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sains</td>\n",
       "      <td>7.000</td>\n",
       "      <td>kg</td>\n",
       "      <td>Bonne Maman Wild Blueberry Conserve</td>\n",
       "      <td>food_cupboard</td>\n",
       "      <td>9.70</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sains</td>\n",
       "      <td>8.900</td>\n",
       "      <td>kg</td>\n",
       "      <td>Dr. Oetker Liquid Glucose</td>\n",
       "      <td>food_cupboard</td>\n",
       "      <td>15.40</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>15.4</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  supermarket  Sains_price  unit                                names  \\\n",
       "0       Sains        6.000  unit                              Vuse Go   \n",
       "1       Sains        6.000  unit                      Vuse Go Edition   \n",
       "2       Sains        5.425    kg                 Heinz Tomato Ketchup   \n",
       "3       Sains        7.000    kg  Bonne Maman Wild Blueberry Conserve   \n",
       "4       Sains        8.900    kg            Dr. Oetker Liquid Glucose   \n",
       "\n",
       "        category  ASDA_price  Morrisons_price  Tesco_price  year  month  day  \\\n",
       "0           home        5.00         5.990000       2250.0  2024      1   29   \n",
       "1           home        3.99         5.990000       2250.0  2024      1   29   \n",
       "2  food_cupboard        4.80         5.666667          3.9  2024      1   29   \n",
       "3  food_cupboard        9.70         7.700000          9.7  2024      1   29   \n",
       "4  food_cupboard       15.40        15.400000         15.4  2024      1   29   \n",
       "\n",
       "   weekday  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in ['supermarket', 'names','unit','category']:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(group, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(group) - sequence_length):\n",
    "        X.append(group.iloc[i:i + sequence_length].drop(columns='unit_price').values)\n",
    "        y.append(group.iloc[i + sequence_length]['unit_price'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 10\n",
    "X_list, y_list = [], []\n",
    "\n",
    "# Group by the product or other unique identifier column\n",
    "for _, group in df.groupby('names'):\n",
    "    X_group, y_group = create_sequences(group, sequence_length)\n",
    "    X_list.append(X_group)\n",
    "    y_list.append(y_group)\n",
    "\n",
    "# Concatenate all groups\n",
    "X = np.concatenate(X_list)\n",
    "y = np.concatenate(y_list)\n",
    "\n",
    "# Ensure correct data types\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 18s 45ms/step - loss: 28.5501 - val_loss: 25.5783\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 26.5422 - val_loss: 27.0769\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 26.1517 - val_loss: 26.0977\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.8747 - val_loss: 25.9389\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 26.1900 - val_loss: 26.1957\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.6182 - val_loss: 26.8730\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 25.9824 - val_loss: 25.6866\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.7385 - val_loss: 26.0378\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.9379 - val_loss: 26.0117\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 6s 40ms/step - loss: 25.7459 - val_loss: 25.9753\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 26.1019 - val_loss: 25.4322\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.4750 - val_loss: 25.2664\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 5s 32ms/step - loss: 25.9374 - val_loss: 25.1365s - loss: 26.\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 5s 31ms/step - loss: 25.8207 - val_loss: 25.5848\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 5s 32ms/step - loss: 25.8362 - val_loss: 25.9586\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 5s 33ms/step - loss: 25.6554 - val_loss: 25.5866\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 5s 33ms/step - loss: 25.6797 - val_loss: 26.4401\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 5s 33ms/step - loss: 25.6234 - val_loss: 25.9477\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.6701 - val_loss: 25.4707\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.3462 - val_loss: 25.7626\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 25.8073 - val_loss: 26.0598\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 5s 30ms/step - loss: 25.7664 - val_loss: 25.1172\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 5s 30ms/step - loss: 25.8692 - val_loss: 25.9630\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 6s 40ms/step - loss: 25.8612 - val_loss: 25.7299\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 6s 40ms/step - loss: 25.9522 - val_loss: 25.9847-\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 25.8216 - val_loss: 26.4131\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.8429 - val_loss: 25.9015\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.7686 - val_loss: 26.1168\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 5s 35ms/step - loss: 25.6399 - val_loss: 25.4516\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 5s 33ms/step - loss: 25.4071 - val_loss: 25.0526\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 5s 29ms/step - loss: 25.6662 - val_loss: 25.5619\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 5s 29ms/step - loss: 25.5884 - val_loss: 25.6688\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 4s 27ms/step - loss: 25.6413 - val_loss: 25.1818\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 6s 35ms/step - loss: 25.4412 - val_loss: 25.5445\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 25.8596 - val_loss: 25.2294\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.4997 - val_loss: 25.4835\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 25.4844 - val_loss: 25.1847\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 9s 54ms/step - loss: 25.6185 - val_loss: 25.2020\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 25.4828 - val_loss: 25.2404\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 9s 60ms/step - loss: 25.5532 - val_loss: 25.3077A: 0s - los\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 25.4277 - val_loss: 25.2914\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.4206 - val_loss: 25.1915\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.3293 - val_loss: 25.3146\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.6197 - val_loss: 25.2471\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.4692 - val_loss: 25.3518\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.6029 - val_loss: 25.6011\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 9s 55ms/step - loss: 25.8364 - val_loss: 25.1805\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.5350 - val_loss: 25.2725\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 25.6178 - val_loss: 25.7932\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.5991 - val_loss: 25.3961\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.5807 - val_loss: 25.2777\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 25.7073 - val_loss: 25.3965\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.7186 - val_loss: 25.6587\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.9781 - val_loss: 25.2693\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 6s 35ms/step - loss: 25.6055 - val_loss: 25.5469\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 6s 36ms/step - loss: 25.6505 - val_loss: 25.6681\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 5s 34ms/step - loss: 25.9799 - val_loss: 25.7356\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 6s 35ms/step - loss: 25.7764 - val_loss: 25.5806\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.5928 - val_loss: 25.5983\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.4640 - val_loss: 25.4616\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 6s 41ms/step - loss: 25.6083 - val_loss: 25.4055\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.6426 - val_loss: 25.8469\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 25.5763 - val_loss: 25.3149\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.7865 - val_loss: 25.3846\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.6003 - val_loss: 25.1976\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.8580 - val_loss: 25.5189\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.6197 - val_loss: 25.4240\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.5954 - val_loss: 25.5160\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.8078 - val_loss: 25.6207\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 6s 40ms/step - loss: 25.9360 - val_loss: 25.7331\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.6395 - val_loss: 25.4490\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 10s 65ms/step - loss: 25.6672 - val_loss: 25.3789\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 25.5164 - val_loss: 25.6301\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.5294 - val_loss: 25.7984\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.5287 - val_loss: 25.4553\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 25.6501 - val_loss: 25.5517\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.9447 - val_loss: 25.7634\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.4369 - val_loss: 25.3950\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 7s 41ms/step - loss: 25.3654 - val_loss: 25.3671\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 7s 41ms/step - loss: 25.6851 - val_loss: 25.5272\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 25.8887 - val_loss: 25.3418\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 6s 36ms/step - loss: 25.5478 - val_loss: 25.3759\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 8s 52ms/step - loss: 25.7079 - val_loss: 25.3326\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.8614 - val_loss: 25.4509\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.6345 - val_loss: 25.3625\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 7s 44ms/step - loss: 25.5709 - val_loss: 25.4737\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 25.6467 - val_loss: 25.5559\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 10s 64ms/step - loss: 25.2445 - val_loss: 25.3818\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 9s 56ms/step - loss: 25.4467 - val_loss: 25.3701\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 7s 46ms/step - loss: 25.5973 - val_loss: 25.3248\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 7s 43ms/step - loss: 25.9435 - val_loss: 25.3503\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 25.6090 - val_loss: 25.4304\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 25.8467 - val_loss: 25.5120\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 25.5358 - val_loss: 25.3631\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 25.5025 - val_loss: 25.1665\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 25.6245 - val_loss: 25.4078\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 26.0367 - val_loss: 25.2302\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 25.4610 - val_loss: 25.2864\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 25.7806 - val_loss: 25.4098\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 7s 42ms/step - loss: 25.5665 - val_loss: 25.2310\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 24.4211\n",
      "Test RMSE: 24.42105484008789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_X,test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure correct data types for train and test sets\n",
    "train_X = train_X.astype(np.float32)\n",
    "train_y = train_y.astype(np.float32)\n",
    "X_test =test_X.astype(np.float32)\n",
    "test_y = test_y.astype(np.float32)\n",
    "\n",
    "# Custom RMSE loss function\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Build the enhanced LSTM model with additional layers\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with Dropout\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer with Dropout\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third LSTM layer with Dropout\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fourth LSTM layer with Dropout\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# First Dense layer\n",
    "model.add(Dense(25, activation='relu'))\n",
    "\n",
    "# Second Dense layer\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model with the custom RMSE loss function\n",
    "model.compile(optimizer='adam', loss=rmse)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X, train_y, epochs=100, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, test_y)\n",
    "print(f\"Test RMSE: {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.604487 ],\n",
       "       [ 9.506513 ],\n",
       "       [10.5390415],\n",
       "       ...,\n",
       "       [10.017068 ],\n",
       "       [ 9.689376 ],\n",
       "       [10.279777 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save('Sains_lstm_model1.h5')\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model('Sains_lstm_model1.h5',compile=False)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save label encoders\n",
    "with open('Sains_label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "\n",
    "# Load label encoders\n",
    "with open('Sains_label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
